{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building GPT from Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Downloading Dataset and reading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('input.txt', 'w') as f:\n",
    "    f.write(response.text)\n",
    "print(\"File downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset in characters is 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the dataset in characters is {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# this is the vocab size that the model has access to.\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Character level tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {char:i for i,char in enumerate(chars)}\n",
    "itos = {i:chars for i, chars in enumerate(chars)}\n",
    "\n",
    "# this is one type of encoding scheme\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda a: ''.join([itos[c] for c in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in practice there are different schema\n",
    "# gpt 2 uses tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype) # this is a 1D tensor\n",
    "print(data[:1000])\n",
    "# torch size will be the same as the length of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting into train and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(text))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not send all the dataset at once, instead we send random batches of dataset one by one to make it less computationally expensive. We call it either *Context Length* or *Block size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling 9 characters\n",
    "# in the context of 18, 47 comes next\n",
    "# in the context of 18, 47 comes 57\n",
    "# so total of 8 examples\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is contexttensor([18]) the target is 47\n",
      "when input is contexttensor([18, 47]) the target is 56\n",
      "when input is contexttensor([18, 47, 56]) the target is 57\n",
      "when input is contexttensor([18, 47, 56, 57]) the target is 58\n",
      "when input is contexttensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is contexttensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is contexttensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is contexttensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "# 8 examples in a chunk of 9 characters\n",
    "# in inference, we sample the characters upto blocksize and truncate everything\n",
    "# after that.\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print(f\"when input is context{context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# we stack up sampled chunks and send them in batches for efficiency to keep\n",
    "# gpus busy for parallel\n",
    "# chunks are processed simultaneously\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8 # max context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split==\"train\" else val_data # data array\n",
    "  ix = torch.randint(len(data)-block_size, (batch_size,)) # random offsetting\n",
    "  x = torch.stack([data[i:i+block_size]for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Baseline: Bigram Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel_v1(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # each token directly reads off the logits for the next token from a lookup table\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integets\n",
    "    # idx is passed into the table and pluck out the row ith represented by idx.\n",
    "    logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    #idx is (B,T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self(idx)\n",
    "      # extracting the last element in the time (context length)\n",
    "      logits = logits[:, -1, :] # becomes (B,C)\n",
    "      # apply softmax to get proba\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      # sample from the distribution\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "      # append sampled index to the running sequence\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel_v1(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation is completely random since its not trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = m(x, y)\n",
    "            losses.append(loss.item())\n",
    "        out[f'{split}_loss'] = sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.994417190551758\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "for steps in range(2000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=False)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLe'w nQGXwf meaithiFosmZAcor$JiBoiZANCZdgbotonYKIOfi:3Z$-ZvZUf etoiSAR\n",
      "tDOM:CO:Cjus\n",
      "RCOFs prTsCHeGTmsrym Ideet'd lake tL!mq-teapetix\n",
      "Cj;wouE; tanshlathi.?&kt3Cd\n",
      "xp. Ag.3fflabjs?'ge?w an$A:xqthatthloT;izoiig hatwfFNCk':po mat;\n",
      "pea:xNtht$3CuFQ!umegoTrWdZDusuolon\n",
      "v:CA:PlicPy.\n",
      "WWR,\n",
      "SGJ$'bod,useremy he.\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is till random since the tokens are not talking to each other and they are only looking at the previous token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "# 8 tokens in a sequence of 4 batches that are not talking to each other\n",
    "# we also do not want the token to communicate with the tokens in the future.\n",
    "# so taking an average of the tokens channels (ie their embeddings) for the current time-step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # lower triangular matrix\n",
    "a = a/torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)\n",
    "# should be true idk why lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # masking and hiding the future tokens\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of each of the token do we need to aggregate is done through the third version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version4: self attention\n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version4: self attention\n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size) # in_features 2 outfeature 16\n",
    "query = nn.Linear(C, head_size) # in features 2 outfeatures 16\n",
    "value = nn.Linear(C, head_size) # in features 2 outfeatures 16\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) # (B, T, 16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v\n",
    "out.shape # (B,T,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.3889e-01, 2.6111e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.6116e-01, 1.0023e-01, 6.3861e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.0119e-01, 3.7609e-02, 2.7604e-01, 5.8516e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.2367e-01, 6.2841e-02, 2.5512e-01, 4.3943e-01, 1.1893e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.3814e-01, 6.0226e-02, 1.7878e-01, 2.1151e-01, 3.5880e-01,\n",
       "          5.2553e-02, 0.0000e+00, 0.0000e+00],\n",
       "         [9.9800e-02, 3.7942e-02, 2.1766e-01, 3.8689e-01, 1.4151e-01,\n",
       "          6.8062e-03, 1.0938e-01, 0.0000e+00],\n",
       "         [1.2532e-01, 8.0922e-02, 1.6263e-01, 1.9613e-01, 1.6991e-01,\n",
       "          5.0191e-02, 1.2560e-01, 8.9304e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [8.7899e-01, 1.2101e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.1789e-01, 3.8315e-01, 4.9896e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.1230e-01, 1.4399e-02, 3.5309e-05, 4.7327e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.4592e-01, 7.1298e-02, 9.2086e-04, 5.5755e-01, 2.4318e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.2833e-01, 2.4246e-01, 4.8739e-02, 3.8327e-01, 4.8419e-02,\n",
       "          4.8771e-02, 0.0000e+00, 0.0000e+00],\n",
       "         [3.6518e-01, 4.6121e-02, 4.4037e-04, 4.9706e-01, 2.9935e-02,\n",
       "          2.4369e-03, 5.8827e-02, 0.0000e+00],\n",
       "         [4.5144e-01, 1.0469e-02, 4.7950e-05, 3.1157e-01, 8.4164e-02,\n",
       "          9.8586e-04, 8.9514e-02, 5.1803e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.1353e-02, 9.7865e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.4583e-02, 7.6278e-01, 1.7264e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.1634e-01, 3.7621e-01, 4.2191e-01, 8.5548e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.2079e-02, 5.4087e-01, 3.0792e-01, 3.3829e-02, 5.5302e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.6606e-02, 6.4671e-01, 1.1738e-01, 5.7188e-03, 2.1273e-02,\n",
       "          1.9231e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [1.0261e-01, 2.4655e-01, 2.7277e-01, 8.1620e-02, 8.0673e-02,\n",
       "          9.7119e-02, 1.1867e-01, 0.0000e+00],\n",
       "         [5.5194e-02, 4.0339e-01, 2.4935e-01, 3.1656e-02, 4.8575e-02,\n",
       "          1.1912e-01, 4.3954e-02, 4.8763e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.3317e-01, 2.6683e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.1245e-01, 2.5009e-01, 5.3746e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.3592e-03, 1.1132e-03, 1.7702e-09, 9.9353e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.8709e-02, 1.0754e-02, 1.4207e-06, 9.4145e-01, 1.9085e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.7844e-02, 2.6772e-02, 2.6508e-05, 8.6384e-01, 4.3471e-02,\n",
       "          8.0461e-03, 0.0000e+00, 0.0000e+00],\n",
       "         [5.6477e-02, 2.3972e-02, 2.3276e-05, 8.5349e-01, 5.4975e-02,\n",
       "          8.0353e-03, 3.0322e-03, 0.0000e+00],\n",
       "         [2.2125e-02, 7.2046e-03, 5.0242e-07, 9.3456e-01, 1.7710e-02,\n",
       "          1.4716e-03, 5.5248e-04, 1.6372e-02]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.6195e-01,  2.0380e-01,  7.1850e-01,  3.2005e-01,  3.6673e-01,\n",
       "          -2.8043e-02, -5.9019e-01, -4.6326e-01, -7.3059e-01,  2.2130e-01,\n",
       "          -6.4916e-01, -5.7867e-01,  4.7627e-01,  1.2566e-01,  1.6999e-01,\n",
       "          -6.3481e-01],\n",
       "         [-8.4593e-01,  1.6441e-01,  7.3828e-01,  2.0597e-01,  4.4652e-01,\n",
       "          -2.1846e-01, -7.5531e-01, -5.6929e-01, -7.7721e-01,  2.6195e-01,\n",
       "          -7.6522e-01, -7.3183e-01,  5.5190e-01,  9.8294e-02,  1.9184e-01,\n",
       "          -5.0693e-01],\n",
       "         [-8.2970e-01,  3.1010e-02,  8.9059e-01,  4.3653e-01,  3.7503e-01,\n",
       "           1.9230e-02, -5.8252e-01, -4.1082e-01, -7.6842e-01,  1.1764e-01,\n",
       "          -6.9481e-01, -4.9016e-01,  4.1679e-01,  2.3586e-02,  3.0479e-01,\n",
       "          -6.3694e-01],\n",
       "         [-8.5588e-01, -1.3270e-01,  1.0644e+00,  6.2489e-01,  3.3970e-01,\n",
       "           1.7561e-01, -4.8269e-01, -2.9569e-01, -7.8318e-01, -1.6218e-02,\n",
       "          -6.7947e-01, -3.1017e-01,  3.1096e-01, -7.0851e-02,  4.3743e-01,\n",
       "          -7.1015e-01],\n",
       "         [-7.7285e-01,  1.0309e-02,  9.2965e-01,  5.8380e-01,  3.0218e-01,\n",
       "           2.1562e-01, -4.2341e-01, -2.9269e-01, -7.3693e-01,  4.4296e-02,\n",
       "          -6.0014e-01, -3.1525e-01,  3.2511e-01,  1.5240e-02,  3.2935e-01,\n",
       "          -7.5888e-01],\n",
       "         [-5.8688e-01,  3.2662e-01,  6.3220e-01,  4.9773e-01,  2.1655e-01,\n",
       "           3.1098e-01, -2.8657e-01, -2.8201e-01, -6.3332e-01,  1.7595e-01,\n",
       "          -4.2105e-01, -3.2056e-01,  3.5336e-01,  2.0580e-01,  9.0608e-02,\n",
       "          -8.7104e-01],\n",
       "         [-7.5332e-01,  5.1054e-02,  8.9008e-01,  5.6365e-01,  2.9617e-01,\n",
       "           2.1493e-01, -4.1653e-01, -2.9892e-01, -7.2603e-01,  6.5397e-02,\n",
       "          -5.8388e-01, -3.2710e-01,  3.3450e-01,  3.9523e-02,  2.9803e-01,\n",
       "          -7.6505e-01],\n",
       "         [-6.2040e-01,  3.5021e-01,  5.9657e-01,  3.9405e-01,  2.6404e-01,\n",
       "           1.7895e-01, -3.9178e-01, -3.6281e-01, -6.5187e-01,  2.3022e-01,\n",
       "          -4.8076e-01, -4.4083e-01,  4.1715e-01,  2.1718e-01,  6.6663e-02,\n",
       "          -7.9063e-01]],\n",
       "\n",
       "        [[-1.0788e+00, -6.4223e-01,  1.5653e+00,  9.2078e-01,  3.9054e-01,\n",
       "           2.4693e-01, -5.1654e-01, -1.8098e-01, -9.0759e-01, -3.0015e-01,\n",
       "          -8.4983e-01, -1.0783e-01,  1.6575e-01, -3.7323e-01,  8.3194e-01,\n",
       "          -6.7299e-01],\n",
       "         [-9.7671e-01, -4.7118e-01,  1.4049e+00,  8.7746e-01,  3.4245e-01,\n",
       "           3.0311e-01, -4.3872e-01, -1.7250e-01, -8.5070e-01, -2.3043e-01,\n",
       "          -7.5057e-01, -1.0672e-01,  1.7897e-01, -2.7009e-01,  7.0304e-01,\n",
       "          -7.3659e-01],\n",
       "         [-1.8237e-01,  1.2559e+00, -2.8194e-01, -4.6317e-02,  1.2626e-01,\n",
       "           1.7461e-01, -2.3005e-01, -4.9491e-01, -4.0761e-01,  6.9579e-01,\n",
       "          -1.1365e-01, -6.9502e-01,  6.2099e-01,  7.5717e-01, -6.2919e-01,\n",
       "          -9.3480e-01],\n",
       "         [-8.9779e-01, -4.5302e-01,  1.4072e+00,  1.0126e+00,  2.5996e-01,\n",
       "           5.0894e-01, -2.6469e-01, -5.4392e-02, -8.0691e-01, -2.8688e-01,\n",
       "          -6.3508e-01,  6.5582e-02,  9.1775e-02, -2.5508e-01,  6.9818e-01,\n",
       "          -8.7086e-01],\n",
       "         [-8.3002e-01, -3.4108e-01,  1.3025e+00,  9.8611e-01,  2.2744e-01,\n",
       "           5.4842e-01, -2.1151e-01, -4.7251e-02, -7.6916e-01, -2.4212e-01,\n",
       "          -5.6870e-01,  6.8642e-02,  9.9229e-02, -1.8752e-01,  6.1394e-01,\n",
       "          -9.1421e-01],\n",
       "         [-6.7982e-01,  4.6031e-02,  9.1651e-01,  7.2191e-01,  2.1063e-01,\n",
       "           4.3787e-01, -2.3254e-01, -1.6746e-01, -6.8529e-01, -8.3963e-03,\n",
       "          -4.6886e-01, -1.3365e-01,  2.3455e-01,  4.1066e-02,  3.1173e-01,\n",
       "          -9.0647e-01],\n",
       "         [-8.8290e-01, -3.8348e-01,  1.3344e+00,  9.4028e-01,  2.7069e-01,\n",
       "           4.5357e-01, -2.9792e-01, -9.6810e-02, -7.9855e-01, -2.3355e-01,\n",
       "          -6.3580e-01, -1.3500e-03,  1.3183e-01, -2.1474e-01,  6.4232e-01,\n",
       "          -8.4681e-01],\n",
       "         [-9.3883e-01, -4.2051e-01,  1.3596e+00,  8.8026e-01,  3.1954e-01,\n",
       "           3.4212e-01, -3.9712e-01, -1.5687e-01, -8.2962e-01, -2.1692e-01,\n",
       "          -7.0942e-01, -8.7131e-02,  1.7298e-01, -2.3908e-01,  6.6584e-01,\n",
       "          -7.6970e-01]],\n",
       "\n",
       "        [[-6.5660e-01,  6.7188e-01,  2.3012e-01, -1.5591e-01,  4.3310e-01,\n",
       "          -3.8562e-01, -8.0132e-01, -7.3991e-01, -6.7145e-01,  5.7543e-01,\n",
       "          -6.4602e-01, -1.0162e+00,  7.3914e-01,  3.9750e-01, -2.0532e-01,\n",
       "          -4.8261e-01],\n",
       "         [-8.5406e-01, -2.2835e-01,  1.1709e+00,  7.7006e-01,  2.9959e-01,\n",
       "           3.1722e-01, -3.8269e-01, -1.9896e-01, -7.8232e-01, -1.1048e-01,\n",
       "          -6.4412e-01, -1.6172e-01,  2.2686e-01, -1.2500e-01,  5.1715e-01,\n",
       "          -7.8499e-01],\n",
       "         [-7.1685e-01,  2.0529e-02,  9.3423e-01,  6.8362e-01,  2.4259e-01,\n",
       "           3.6547e-01, -2.9724e-01, -2.0627e-01, -7.0586e-01,  1.6600e-03,\n",
       "          -5.1728e-01, -1.8897e-01,  2.6095e-01,  2.4385e-02,  3.2812e-01,\n",
       "          -8.5615e-01],\n",
       "         [-4.9573e-01,  4.7513e-01,  4.9365e-01,  4.6518e-01,  1.7200e-01,\n",
       "           3.6700e-01, -2.1300e-01, -2.7040e-01, -5.8256e-01,  2.3416e-01,\n",
       "          -3.3107e-01, -3.1336e-01,  3.6163e-01,  2.9550e-01, -2.0985e-02,\n",
       "          -9.3087e-01],\n",
       "         [-6.1748e-01,  2.4560e-01,  7.1324e-01,  5.5474e-01,  2.1912e-01,\n",
       "           3.3657e-01, -2.8013e-01, -2.5541e-01, -6.5042e-01,  1.2624e-01,\n",
       "          -4.4066e-01, -2.7610e-01,  3.2394e-01,  1.5800e-01,  1.5397e-01,\n",
       "          -8.7422e-01],\n",
       "         [-9.1463e-01, -2.0588e-01,  1.1288e+00,  6.1255e-01,  3.7738e-01,\n",
       "           1.0740e-01, -5.5263e-01, -3.2523e-01, -8.1587e-01, -3.1935e-02,\n",
       "          -7.4514e-01, -3.4870e-01,  3.2489e-01, -1.1588e-01,  4.9063e-01,\n",
       "          -6.5476e-01],\n",
       "         [-6.0784e-01,  4.5679e-01,  4.8212e-01,  2.6222e-01,  2.9214e-01,\n",
       "           6.3987e-02, -4.6767e-01, -4.4548e-01, -6.4474e-01,  3.2157e-01,\n",
       "          -4.9766e-01, -5.6934e-01,  4.9187e-01,  2.7839e-01, -2.0274e-02,\n",
       "          -7.3457e-01],\n",
       "         [-7.1986e-01,  1.9046e-01,  7.4523e-01,  4.2618e-01,  3.1357e-01,\n",
       "           1.1456e-01, -4.7434e-01, -3.7772e-01, -7.0726e-01,  1.6891e-01,\n",
       "          -5.7973e-01, -4.5213e-01,  4.1007e-01,  1.2061e-01,  1.8653e-01,\n",
       "          -7.2363e-01]],\n",
       "\n",
       "        [[-1.5039e+00, -1.2562e+00,  2.1244e+00,  9.5638e-01,  6.2956e-01,\n",
       "          -1.2623e-01, -9.3812e-01, -3.1200e-01, -1.1442e+00, -4.9558e-01,\n",
       "          -1.2962e+00, -2.5955e-01,  1.9430e-01, -7.4692e-01,  1.2870e+00,\n",
       "          -3.3531e-01],\n",
       "         [-1.4100e+00, -1.1153e+00,  1.9951e+00,  9.4065e-01,  5.7890e-01,\n",
       "          -5.1422e-02, -8.5036e-01, -2.8828e-01, -1.0919e+00, -4.4729e-01,\n",
       "          -1.1994e+00, -2.3405e-01,  1.9254e-01, -6.6140e-01,  1.1821e+00,\n",
       "          -4.0589e-01],\n",
       "         [-1.7574e-01,  1.0079e+00, -5.4328e-03,  3.3404e-01,  2.0100e-02,\n",
       "           5.4742e-01,  3.3901e-02, -2.4080e-01, -4.0431e-01,  4.4958e-01,\n",
       "          -1.9041e-02, -3.0524e-01,  4.0042e-01,  6.1686e-01, -4.2224e-01,\n",
       "          -1.1324e+00],\n",
       "         [-2.2696e+00, -2.5981e+00,  3.3930e+00,  1.3694e+00,  9.6634e-01,\n",
       "          -4.6236e-01, -1.4619e+00, -3.1715e-01, -1.5708e+00, -1.0760e+00,\n",
       "          -2.0200e+00, -1.7805e-01,  4.4131e-02, -1.5540e+00,  2.3029e+00,\n",
       "           9.6856e-02],\n",
       "         [-2.2323e+00, -2.5284e+00,  3.3264e+00,  1.3428e+00,  9.5172e-01,\n",
       "          -4.5230e-01, -1.4408e+00, -3.2122e-01, -1.5501e+00, -1.0435e+00,\n",
       "          -1.9863e+00, -1.8865e-01,  5.5206e-02, -1.5122e+00,  2.2499e+00,\n",
       "           7.9139e-02],\n",
       "         [-2.1707e+00, -2.4149e+00,  3.2182e+00,  1.3013e+00,  9.2685e-01,\n",
       "          -4.3321e-01, -1.4043e+00, -3.2627e-01, -1.5157e+00, -9.9140e-01,\n",
       "          -1.9300e+00, -2.0360e-01,  7.2052e-02, -1.4441e+00,  2.1635e+00,\n",
       "           4.8541e-02],\n",
       "         [-2.1632e+00, -2.3997e+00,  3.2035e+00,  1.2944e+00,  9.2426e-01,\n",
       "          -4.3256e-01, -1.4009e+00, -3.2806e-01, -1.5115e+00, -9.8383e-01,\n",
       "          -1.9234e+00, -2.0723e-01,  7.5148e-02, -1.4351e+00,  2.1519e+00,\n",
       "           4.5663e-02],\n",
       "         [-2.2226e+00, -2.5137e+00,  3.3128e+00,  1.3408e+00,  9.4657e-01,\n",
       "          -4.4488e-01, -1.4320e+00, -3.1899e-01, -1.5447e+00, -1.0383e+00,\n",
       "          -1.9764e+00, -1.8635e-01,  5.5215e-02, -1.5033e+00,  2.2388e+00,\n",
       "           7.2004e-02]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, C)\n",
    "q = torch.randn(B, T, C)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # scaling is used to control the initial variance of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7433)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 21 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout=0.0\n",
    "torch.manual_seed(1337)\n",
    "# ------------\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C) where c is the head size\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # (B,T, num_heads*head_size) -> (B,T,n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # projection is the linear outcome of the concatenation of the heads\n",
    "        return self.proj(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    ''' Transformer block: coomunication followed by computation '''\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection.\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, 4),\n",
    "            Block(n_embd, 4),\n",
    "            Block(n_embd, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train_loss']:.4f}, val loss {losses['val_loss']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
