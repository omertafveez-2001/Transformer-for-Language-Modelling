# Transformers-for-Language-Modelling
Transformers coded from scratch inspired by the works of `"Self Attention is all you need," Vaswani 2017`. This version of Transformers is built in the same way as described in the paper using **Multi-Headed Self Attention**
