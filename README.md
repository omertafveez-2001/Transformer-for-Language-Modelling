# Transformers-for-Language-Modelling
This repo consists of different codebases of transformers and their applications in recent Large Language Models, such as GPT, GPT2, etc. These codebases are coded and trained from scratch and replicated from different papers and authors. 

**Why do you need this repo?** ðŸ¤”  <br>-
There is not much documented, not good enough, if it is, to follow through and understand each bit of code so that people (me) can reproduce something of our own. This repo has (overstating because if not, it will be updated) all the mechanisms in the most advanced and sought-after papers and language models.

## What's in the repository so far ðŸš€


